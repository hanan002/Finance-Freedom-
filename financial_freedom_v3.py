# -*- coding: utf-8 -*-
"""Financial_Freedom_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V6M2hkfx8aF7A1HNMmf8ylB9lYEABRiv

**Financial Freedom**
"""

import pandas as pd
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

from google.colab import drive
drive.mount('/content/gdrive')

data1 = pd.read_csv('/content/gdrive/MyDrive/ML_Projects/Bitcoin History2.csv')
df = pd.DataFrame(data1)

# แปลงข้อมูลวันที่เป็น datetime
df['Date'] = pd.to_datetime(df['Date'])

# แยกองค์ประกอบของวันที่
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

# สร้าง Feature ฤดูกาล
df['Season'] = np.where(df['Month'].isin([12, 1, 2]), 'Winter',
                        np.where(df['Month'].isin([3, 4, 5]), 'Spring',
                                np.where(df['Month'].isin([6, 7, 8]), 'Summer', 'Autumn')))

# Change the type
cols = ['Price', 'Open', 'High', 'Low']
for column in cols:
    df[column] = df[column].str.replace(',', '').astype(float)

# Vol. tranform

def trans_vol(volume):
    if volume == '-':
        return 0  #for non-numeric values
    elif volume.endswith('K'):
        return float(volume.replace('K', '')) * 1e3
    elif volume.endswith('M'):
        return float(volume.replace('M', '')) * 1e6
    elif volume.endswith('B'):
        return float(volume.replace('B', '')) * 1e9
    else:
        return float(volume)

df['Vol.'] = df['Vol.'].apply(trans_vol)

df['Change %'] = df['Change %'].astype(str).str.replace('%', '').astype(float)

df.sample(10)

df.dtypes

"""# **3.ชุดข้อมูล**"""

a,b = df.shape
print("จำนวนข้อมูล",a)
print("จำนวน Feature",b)

df.columns

df.describe()

"""count: จำนวนข้อมูลที่ไม่ใช่ค่าว่าง

mean: ค่าเฉลี่ย

std: ส่วนเบี่ยงเบนมาตรฐาน

min: ค่าต่ำสุด

25%: ควอไทล์ที่ 1 (percentile ที่ 25)

50%: ควอไทล์ที่ 2 (มัธยฐาน, percentile ที่ 50)

75%: ควอไทล์ที่ 3 (percentile ที่ 75)

max: ค่าสูงสุด

## 3.4 จัดการข้อมูล (data cleansing & Transformation)

### จัดการ (missing value)
"""

df.isnull().sum()

"""ไม่ปรากำ ข้อมูลสูญหาย(outlier)

### จัดการ (Outlier)

จาก features ['Date', 'Price', 'Open', 'High', 'Low', 'Vol.', 'Change %', 'Year', 'Month', 'Day', 'Season']

features ที่ควรพิจารณากำจัด outlier คือ

1.Price, Open, High, Low: ราคาของ Bitcoin มีความผันผวนสูง จึงมีโอกาสที่จะมี outlier ซึ่งอาจส่งผลต่อ model ได้

2.Vol.: ปริมาณการซื้อขายก็มีความผันผวนเช่นกัน และอาจมี outlier จากเหตุการณ์หรือข่าวสารสำคัญ

3.Change %: เปอร์เซ็นต์การเปลี่ยนแปลงราคามีแนวโน้มที่จะมี outlier โดยเฉพาะในช่วงที่มีการเปลี่ยนแปลงราคาอย่างรวดเร็ว

features ที่ไม่ควรพิจารณากำจัด outlier:

Date, Year, Month, Day, Season: features เหล่านี้เป็นข้อมูลเชิงเวลาและฤดูกาล ซึ่งมีความสำคัญในการวิเคราะห์แนวโน้มของราคา Bitcoin การกำจัด outlier อาจทำให้สูญเสียข้อมูลสำคัญไป

Price
"""

sns.boxplot(df.Price)

# IQR
q1 = df.Price.quantile(.25)
q3 = df.Price.quantile(.75)

iqr= q3 - q1

upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)

print('Q1 = {}'.format(q1))
print('Q3 = {}'.format(q3))
print('IQR = {}'.format(iqr))
print('Upper = {:.2f}'.format(upper))
print('Lower = {}'.format(lower))

df[df.Price > upper].Price.count()

df[df.Price < lower].Price.count()

#กำจัด outlier
df.drop(df[df.Price > upper].index, inplace=True)

"""Open"""

sns.boxplot(df.Open)

# IQR
q1 = df.Open.quantile(.25)
q3 = df.Open.quantile(.75)

iqr= q3 - q1

upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)

print('Q1 = {}'.format(q1))
print('Q3 = {}'.format(q3))
print('IQR = {}'.format(iqr))
print('Upper = {:.2f}'.format(upper))
print('Lower = {}'.format(lower))

df[df.Open > upper].Open.count()

df[df.Open < lower].Open.count()

#กำจัด outlier
df.drop(df[df.Open > upper].index, inplace=True)

"""High"""

sns.boxplot(df.High)

# IQR
q1 = df.High.quantile(.25)
q3 = df.High.quantile(.75)

iqr= q3 - q1

upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)

print('Q1 = {}'.format(q1))
print('Q3 = {}'.format(q3))
print('IQR = {}'.format(iqr))
print('Upper = {:.2f}'.format(upper))
print('Lower = {}'.format(lower))

df[df.High > upper].High.count()

df[df.High < lower].High.count()

#กำจัด outlier
df.drop(df[df.High > upper].index, inplace=True)

"""Low"""

sns.boxplot(df.Low)

# IQR
q1 = df.Low.quantile(.25)
q3 = df.Low.quantile(.75)

iqr= q3 - q1

upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)

print('Q1 = {}'.format(q1))
print('Q3 = {}'.format(q3))
print('IQR = {}'.format(iqr))
print('Upper = {:.2f}'.format(upper))
print('Lower = {}'.format(lower))

df[df.Low > upper].Low.count()

df[df.Low < lower].Low.count()

#กำจัด outlier
df.drop(df[df.Low > upper].index, inplace=True)

"""Vol."""

sns.boxplot(df['Vol.'])

# IQR
q1 = df['Vol.'].quantile(.25)
q3 = df['Vol.'].quantile(.75)

iqr= q3 - q1

upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)

print('Q1 = {}'.format(q1))
print('Q3 = {}'.format(q3))
print('IQR = {}'.format(iqr))
print('Upper = {:.2f}'.format(upper))
print('Lower = {}'.format(lower))

df[df['Vol.'] > upper]['Vol.'].count()

df[df['Vol.'] < lower]['Vol.'].count()

#กำจัด outlier
df.drop(df[df['Vol.'] > upper].index, inplace=True)

"""Change %"""

sns.boxplot(df['Change %'])

# IQR
q1 = df['Change %'].quantile(.25)
q3 = df['Change %'].quantile(.75)

iqr= q3 - q1

upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)

print('Q1 = {}'.format(q1))
print('Q3 = {}'.format(q3))
print('IQR = {}'.format(iqr))
print('Upper = {:.2f}'.format(upper))
print('Lower = {}'.format(lower))

df[df['Change %'] > upper]['Change %'].count()

df[df['Change %'] < lower]['Change %'].count()

#กำจัด outlier
df.drop(df[df['Change %'] > upper].index, inplace=True)

#กำจัด outlier
df.drop(df[df['Change %'] < lower].index, inplace=True)

"""### Transformation

**Date**

**Vol.**

เหตุผล: ข้อมูลปริมาณการซื้อขายมักมีการกระจายตัวแบบ Skewed และมีค่าต่างกันมาก การทำ Transformation จะช่วยลดผลกระทบจากค่าที่ต่างกันมากเกินไป

**Change %**

เหตุผล: เปอร์เซ็นต์การเปลี่ยนแปลงอาจมีค่า outlier และมีการกระจายตัวไม่สม่ำเสมอ
"""

df = pd.get_dummies(df, dtype=int)
df.head()

"""### Drop feature ที่ไม่จำเป็น

มีการสร้าง features Year, Month, Day, Season ซึ่งแสดงข้อมูลเวลาอย่างละเอียดอยู่แล้ว feature Date อาจซ้ำซ้อนและไม่จำเป็น
"""

df = df.drop('Date', axis=1)

df.sample(10)

"""### ทำให้ปกติ(Normalization)

การทำ Normalization นั้นเป็นกระบวนการที่สำคัญในการเตรียมข้อมูลสำหรับการวิเคราะห์ข้อมูล โดยเฉพาะอย่างยิ่งเมื่อคุณกำลังทำงานกับโมเดล Machine Learning ที่ไวต่อสเกลของข้อมูล (เช่น Linear Regression, Neural Networks)

คุณสมบัติที่ควรทำ Normalization ก็คือ คุณสมบัติที่:

มีหน่วยวัดที่แตกต่างกัน: เช่น อายุ (ปี), รายได้ (บาท), ระยะทาง (กิโลเมตร) มีช่วงค่าที่แตกต่างกันมาก: เช่น ค่าที่อยู่ในช่วง 0-100 กับค่าที่อยู่ในช่วง 1000-10000 มีค่าผิดปกติ (Outliers): ค่าที่แตกต่างจากค่าอื่นๆ อย่างมาก เหตุผลที่ต้องทำ Normalization:

เพื่อให้โมเดลเรียนรู้ได้ดีขึ้น: เมื่อคุณสมบัติต่างๆ มีสเกลที่ใกล้เคียงกัน โมเดลจะสามารถเรียนรู้ความสัมพันธ์ระหว่างคุณสมบัติได้ง่ายขึ้น เพื่อให้การปรับพารามิเตอร์ของโมเดลเป็นไปอย่างมีประสิทธิภาพ: อัลกอริทึมการปรับพารามิเตอร์หลายๆ อัลกอริทึมจะทำงานได้ดีขึ้นเมื่อข้อมูลถูกทำให้เป็นมาตรฐาน เพื่อให้สามารถเปรียบเทียบค่าระหว่างคุณสมบัติได้: เมื่อคุณสมบัติต่างๆ มีสเกลที่ใกล้เคียงกัน คุณสามารถเปรียบเทียบความสำคัญของแต่ละคุณสมบัติได้ง่ายขึ้น

จาก features ทั้งหมดที่มี features ที่ควรทำ Normalization มีดังนี้

**Price, Open, High, Low**: ราคาของ Bitcoin มีช่วงค่าที่แตกต่างกันมากในแต่ละช่วงเวลา การทำ Normalization จะช่วยลดผลกระทบจากค่าที่ต่างกันมากเกินไป ทำให้โมเดลเรียนรู้ได้ดีขึ้น

**Vol.**: ปริมาณการซื้อขาย Bitcoin มีค่าที่แตกต่างกันมากในแต่ละวัน และมีการกระจายตัวแบบ Skewed การทำ Normalization จะช่วยลดผลกระทบจากค่าที่ต่างกันมากเกินไป และทำให้โมเดลเรียนรู้ได้ดีขึ้น

**Change %**: เปอร์เซ็นต์การเปลี่ยนแปลงราคามีค่าที่อยู่ในช่วง -100 ถึง 100 การทำ Normalization จะช่วยให้ค่าอยู่ในช่วงที่เหมาะสม และทำให้โมเดลเรียนรู้ได้ดีขึ้น

**เหตุผลที่ features เหล่านี้ควรทำ Normalization:**



**ลดผลกระทบจากค่าที่ต่างกันมากเกินไป**: features เหล่านี้มีช่วงค่าที่แตกต่างกันมาก ซึ่งอาจทำให้โมเดลให้ความสำคัญกับ features ที่มีค่าสูงมากกว่า features ที่มีค่าต่ำ การทำ Normalization จะช่วยลดผลกระทบนี้ ทำให้โมเดลเรียนรู้ได้ดีขึ้น

**ปรับปรุงประสิทธิภาพของโมเดล**: การทำ Normalization จะช่วยปรับปรุงประสิทธิภาพของโมเดล Machine Learning หลายๆ โมเดล เช่น Linear Regression, Neural Networks
**ทำให้สามารถเปรียบเทียบค่าระหว่าง features ได้**: การทำ Normalization จะช่วยให้สามารถเปรียบเทียบค่าระหว่าง features ได้ง่ายขึ้น เนื่องจาก features จะมีสเกลที่ใกล้เคียงกัน



**features ที่ไม่ควรทำ Normalization**:



**Date, Year, Month, Day, Season**: features เหล่านี้เป็นข้อมูลเชิงเวลาและฤดูกาล ซึ่งไม่จำเป็นต้องทำ Normalization เนื่องจากค่าของ features เหล่านี้มีความหมายในตัวเองอยู่แล้ว
"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# เลือก features ที่ต้องการทำ Normalization
features_to_normalize = ['Price', 'Open', 'High', 'Low', 'Vol.', 'Change %']

# สร้าง scaler object
# MinMaxScaler: ทำให้ค่าอยู่ในช่วง 0-1
# StandardScaler: ทำให้ค่ามีค่าเฉลี่ย 0 และส่วนเบี่ยงเบนมาตรฐาน 1
scaler = MinMaxScaler() # or scaler = StandardScaler()

# ทำ Normalization
df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])

df.sample(10)

"""## 3.5. อธิบายการเตรียม Features (X) และ Target (y)

Features (X)

X คือตัวแปรที่เก็บ features หรือตัวแปรอิสระ ที่จะใช้ในการทำนาย target

ในโค้ดนี้ features ที่เลือกใช้คือ:

Open, High, Low: ราคาเปิด, สูงสุด, ต่ำสุดของ Bitcoin ในแต่ละวัน
Vol.: ปริมาณการซื้อขาย Bitcoin ในแต่ละวัน

Change %: เปอร์เซ็นต์การเปลี่ยนแปลงของราคา Bitcoin ในแต่ละวัน

Year, Month, Day: ปี, เดือน, วัน ที่บันทึกข้อมูล

Season_Autumn, Season_Spring, Season_Summer, Season_Winter: features ที่ได้จากการทำ One-Hot Encoding ของ column 'Season' เพื่อแทนฤดูกาลต่างๆ

Target (y)

y คือตัวแปรที่เก็บ target หรือตัวแปรตาม ที่เราต้องการทำนาย

ในโค้ดนี้ target คือ Price ซึ่งเป็นราคาของ Bitcoin
"""

from sklearn.model_selection import train_test_split

X = df[['Open', 'High', 'Low', 'Vol.', 'Change %', 'Year','Month', 'Day', 'Season_Autumn', 'Season_Spring', 'Season_Summer','Season_Winter']]
y = df['Price']

"""## 3.6. เตรียมข้อมูล Train และ Test บันทึกเก็บลงไฟล์ csv"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# บันทึกข้อมูล Train ลงไฟล์ CSV
X_train.to_csv("X_train.csv", index=False)
y_train.to_csv("y_train.csv", index=False)

# บันทึกข้อมูล Test ลงไฟล์ CSV
X_test.to_csv("X_test.csv", index=False)
y_test.to_csv("y_test.csv", index=False)

"""# **4.วิธีดำเนินการ**

## 4.1. เลือกอัลกอริทึมที่เหมาะสมและนักศึกษาสนใจจะศึกษามาอย่างน้อย 2 อัลกอริทึม

Linear Regression

Polynomial Regression

Random Forest
"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error

"""Linear Regression"""

# สร้างโมเดล Linear Regression
model_lineaar = LinearRegression()
model_lineaar.fit(X_train, y_train)

"""Polynomial Regression"""

from sklearn.pipeline import make_pipeline
# สร้าง pipeline ด้วย PolynomialFeatures และ LinearRegression
model_polynomial = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())
model_polynomial.fit(X_train, y_train)

"""Random Forest"""

model_forest = RandomForestRegressor(random_state=42)
model_forest.fit(X_train, y_train)

"""## 4.2. การกำหนด hyperparameter ในแต่ละอัลกอริทึม พร้อมบอกเซตของ candidate ของแต่ละ พารามิเตอร์

Linear Regression
"""

# กำหนดค่า hyperparameter ที่ต้องการทดลอง (สำหรับ Linear Regression อาจมี hyperparameter น้อยกว่า Decision Tree)
param_grid = {
    'fit_intercept': [True, False]
}

# ใช้ GridSearchCV เพื่อค้นหาค่า hyperparameter ที่ดีที่สุด
grid_search_Lregression = GridSearchCV(model_lineaar, param_grid=param_grid, cv=5)
grid_search_Lregression.fit(X_train, y_train)

model_Lr = grid_search_Lregression.best_estimator_

# แสดงผลลัพธ์
print("Best parameters:", grid_search_Lregression.best_params_)

"""Polynomial Regression

"""

# กำหนดค่า hyperparameter ที่ต้องการทดลอง (สำหรับ Linear Regression อาจมี hyperparameter น้อยกว่า Decision Tree)
param_grid = {
    'polynomialfeatures__degree': [1, 2, 3, 4],
    'linearregression__fit_intercept': [True, False]
}

# ใช้ RandomizedSearchCVเพื่อค้นหาค่า hyperparameter ที่ดีที่สุด
grid_search_Pregression = GridSearchCV(model_polynomial, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search_Pregression.fit(X_train, y_train)

model_Pr = grid_search_Pregression.best_estimator_

# แสดงผลลัพธ์
print("Best parameters:", grid_search_Pregression)

"""
Random Forest"""

# กำหนด hyperparameter space
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search_forest = GridSearchCV(model_forest, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search_forest.fit(X_train, y_train)

model_Rf = grid_search_forest.best_estimator_

# แสดงผลลัพธ์
print("Best parameters:", grid_search_forest)

"""## 4.3. วิธีการประเมิน (Evaluation matrices) ที่เลือกใช้และเหมาะสม

MSE(Mean squared error)

MAE(Mean absolute error)

RMSE(Root mean squared error)

R-squared

การประเมินโมเดล: เข้าใจ MSE, MAE, RMSE และ R-squared ในการสร้างโมเดล Machine Learning นั้น สิ่งสำคัญคือการประเมินประสิทธิภาพของโมเดลว่าดีเพียงใด เมตริกต่างๆ ที่ใช้ในการประเมินนี้จะช่วยให้เราเข้าใจว่าโมเดลของเราสามารถทำนายค่าจริงได้แม่นยำแค่ไหน และควรปรับปรุงโมเดลในส่วนใดบ้าง
เมตริกที่ใช้ในการประเมินโมเดลที่พบบ่อย ได้แก่:

MSE (Mean Squared Error):
คำนวณ: หาค่าความต่างระหว่างค่าที่ทำนายได้กับค่าจริง ยกกำลังสอง แล้วนำมาหาค่าเฉลี่ย
ความสำคัญ: MSE ให้ความสำคัญกับค่าผิดพลาดที่มาก โดยค่าผิดพลาดที่มากจะถูกเน้นหนักกว่าค่าผิดพลาดที่น้อย เนื่องจากการยกกำลังสองจะทำให้ค่าผิดพลาดที่มากมีค่าเพิ่มขึ้นอย่างรวดเร็ว
เหมาะสำหรับ: โมเดลที่ต้องการลงโทษค่าผิดพลาดที่มากเป็นพิเศษ

MAE (Mean Absolute Error):
คำนวณ: หาค่าความต่างระหว่างค่าที่ทำนายได้กับค่าจริง แล้วนำมาหาค่าเฉลี่ยของค่าสัมบูรณ์
ความสำคัญ: MAE ให้ความสำคัญกับค่าผิดพลาดโดยเฉลี่ยทั้งหมด โดยไม่เน้นค่าผิดพลาดที่มากเป็นพิเศษ
เหมาะสำหรับ: โมเดลที่ต้องการวัดค่าผิดพลาดโดยเฉลี่ยโดยรวม

RMSE (Root Mean Squared Error):
คำนวณ: คำนวณหาค่า MSE ก่อน แล้วจึงดึงรากที่สองของค่า MSE
ความสำคัญ: RMSE มีหน่วยเดียวกับค่าจริง ทำให้ตีความผลได้ง่ายกว่า MSE และยังให้ความสำคัญกับค่าผิดพลาดที่มากเช่นเดียวกับ MSE
เหมาะสำหรับ: โมเดลที่ต้องการวัดค่าผิดพลาดโดยเฉลี่ย และต้องการให้ค่าผิดพลาดที่มากมีผลกระทบต่อค่า RMSE มากกว่า

R-squared:
คำนวณ: วัดสัดส่วนของความแปรปรวนของตัวแปรตามที่สามารถอธิบายได้ด้วยโมเดล
ความสำคัญ: บอกให้เราทราบว่าโมเดลของเราสามารถอธิบายข้อมูลได้ดีแค่ไหน ค่า R-squared ที่ใกล้ 1 หมายถึงโมเดลสามารถอธิบายข้อมูลได้ดีมาก
เหมาะสำหรับ: การประเมินความสามารถของโมเดลในการอธิบายความแปรปรวนของข้อมูล
สรุป
MSE, RMSE: เหมาะสำหรับการวัดขนาดของค่าผิดพลาด โดยเฉพาะอย่างยิ่งสำหรับโมเดลที่ต้องการลงโทษค่าผิดพลาดที่มาก MAE: เหมาะสำหรับการวัดค่าผิดพลาดโดยเฉลี่ยโดยรวม R-squared: เหมาะสำหรับการวัดความสามารถของโมเดลในการอธิบายข้อมูล
"""

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import root_mean_squared_error
from sklearn.metrics import r2_score

"""Linear regression"""

# ทำนายผลลัพธ์สำหรับ Linear regression
y_pred_Lr = model_Lr.predict(X_test)

# คำนวณ MSE(Mean squared error) MAE(Mean absolute error) RMSE(Root mean squared error) และ R2 สำหรับ Linear regression
MSE_Lr = mean_squared_error(y_test, y_pred_Lr)
MAE_Lr = mean_absolute_error(y_test, y_pred_Lr)
RMSE_Lr = root_mean_squared_error(y_test, y_pred_Lr)
R2_Lr = r2_score(y_test, y_pred_Lr)

print("Linear regression:")
print("MSE: ", MSE_Lr)
print("MAE: ", MAE_Lr)
print("RMSE: ", RMSE_Lr)
print("R2: ",R2_Lr)

"""Polynomial Regression"""

# ทำนายผลลัพธ์สำหรับ Polynomial Regression
y_pred_Pr = model_Pr.predict(X_test)

# คำนวณ MSE(Mean squared error) MAE(Mean absolute error) RMSE(Root mean squared error) และ R2 สำหรับ Polynomial Regression
MSE_Pr = mean_squared_error(y_test, y_pred_Pr)
MAE_Pr = mean_absolute_error(y_test, y_pred_Pr)
RMSE_Pr = root_mean_squared_error(y_test, y_pred_Pr)
R2_Pr = r2_score(y_test, y_pred_Pr)

print("Polynomial Regression:")
print("MSE: ", MSE_Pr)
print("MAE: ", MAE_Pr)
print("RMSE: ", RMSE_Pr)
print("R2: ",R2_Pr)

"""Random Forest"""

# ทำนายผลลัพธ์สำหรับ Polynomial Regression
y_pred_Pr = model_Rf.predict(X_test)

# คำนวณ MSE(Mean squared error) MAE(Mean absolute error) RMSE(Root mean squared error) และ R2 สำหรับ Polynomial Regression
MSE_Rf = mean_squared_error(y_test, y_pred_Pr)
MAE_Rf = mean_absolute_error(y_test, y_pred_Pr)
RMSE_Rf = root_mean_squared_error(y_test, y_pred_Pr)
R2_Rf = r2_score(y_test, y_pred_Pr)

print("Polynomial Regression:")
print("MSE: ", MSE_Rf)
print("MAE: ", MAE_Rf)
print("RMSE: ", RMSE_Rf)
print("R2: ",R2_Rf)

"""## 4.4. สร้างโมเดลพร้อมบันทึกผลการประเมิน"""

#โมเดลพร้อมบันทึกผลการประเมิน
evaluation_results = pd.DataFrame(columns=['Model', 'MSE', 'MAE','RMSE','R2'])

"""Linear regression"""

# เพิ่มผลการประเมินของ Linear regression
evaluation_results = pd.concat([evaluation_results, pd.DataFrame({
    'Model': ['Linear regression'],
    'MSE': [MSE_Lr],
    'MAE': [MAE_Lr],
    'RMSE': [RMSE_Lr],
    'R2': [R2_Lr]
})], ignore_index=True)

"""Polynomial Regression"""

# เพิ่มผลการประเมินของ Polynomial Regression
evaluation_results = pd.concat([evaluation_results, pd.DataFrame({
    'Model': ['Polynomial Regression'],
    'MSE': [MSE_Pr],
    'MAE': [MAE_Pr],
    'RMSE': [RMSE_Pr],
    'R2': [R2_Pr]
})], ignore_index=True)

"""Random Forest"""

# เพิ่มผลการประเมินของ Random Forest
evaluation_results = pd.concat([evaluation_results, pd.DataFrame({
    'Model': ['Random Forest'],
    'MSE': [MSE_Rf],
    'MAE': [MAE_Rf],
    'RMSE': [RMSE_Rf],
    'R2': [R2_Rf]
})], ignore_index=True)

evaluation_results.to_csv("model_evaluation.csv", index=False)

"""## 4.5. เลือกโมเดลที่ดีที่สุด บันทึกไฟล์ model เก็บไป เพื่อนำไป Deploy

โมเดล Polynomial Regression มีค่าสถิติทั้งหมดต่ำกว่าโมเดลอื่นๆ ทั้งหมด ซึ่งหมายความว่าโมเดล Polynomial Regression สามารถทำนายค่าได้แม่นยำที่สุด และสามารถอธิบายความสัมพันธ์ระหว่างตัวแปรได้ดีที่สุด
"""

import joblib

# บันทึก model ที่ดีที่สุด
joblib.dump(model_Pr, 'best_model_bitcoin.pkl')

"""# **5. ผลการทดลอง**

## 5.1. แสดงเป็นตาราง หรือกราฟ ที่ดูเข้าใจง่าย

**Test1**
"""

from sklearn.model_selection import cross_val_score

# Calculating cross-validated scores for each model
Lr_scores = cross_val_score(model_Lr, X_train, y_train, cv=5)
Pr_scores = cross_val_score(model_Pr, X_train, y_train, cv=5)
Rf_scores = cross_val_score(model_Rf, X_train, y_train, cv=5)

# Printing the mean cross-validated scores
print("Linear Regression Mean Score:", Lr_scores.mean())
print("Polynomial Regression Mean Score:", Pr_scores.mean())
print("Random Forest Regression Mean Score:", Rf_scores.mean())


# Comparing the scores and choose the best model based on the highest mean score
if Lr_scores.mean() > Pr_scores.mean()\
 and Lr_scores.mean() > Rf_scores.mean():
  print("Linear Regression is the best model.")
elif Pr_scores.mean() > Lr_scores.mean()\
 and Pr_scores.mean() > Rf_scores.mean():
  print("Polynomial Regression is the best model.")
else:
  print("Random Forest Regression is the best model.")

"""**test 2**"""

import matplotlib.pyplot as plt
import numpy as np

# Performance metrics for each model
models = ['Linear Regression', 'Polynomial Regression', 'Random Forest Regression']
MSE_values = [MSE_Lr, MSE_Pr, MSE_Rf]
MAE_values = [MAE_Lr, MAE_Pr, MAE_Rf]
RMSE_values = [RMSE_Lr, RMSE_Pr, RMSE_Rf]
R2_values = [R2_Lr, R2_Pr, R2_Rf]

# Create Mean squared error plot
fig, ax_r2 = plt.subplots(figsize=(8, 5))
ax_r2.bar(models, MSE_values, color='navy')
ax_r2.set_title('MSE(Mean squared error)')
ax_r2.set_ylabel('MSE')
plt.tight_layout()
plt.show()

# Create Mean absolute error plot
fig, ax_r2 = plt.subplots(figsize=(8, 5))
ax_r2.bar(models, MAE_values, color='maroon')
ax_r2.set_title('MAE(Mean absolute error)')
ax_r2.set_ylabel('MAE')
plt.tight_layout()
plt.show()

# Create RMSE plot
fig, ax_rmse = plt.subplots(figsize=(8, 5))
ax_rmse.bar(models, RMSE_values, color='grey')
ax_rmse.set_title('(RMSE)Root Mean Squared Error')
ax_rmse.set_ylabel('RMSE')
plt.tight_layout()
plt.show()

# Create RMSE plot
fig, ax_rmse = plt.subplots(figsize=(8, 5))
ax_rmse.bar(models, R2_values, color='violet')
ax_rmse.set_title('R²(R-squared)')
ax_rmse.set_ylabel('R²')
plt.tight_layout()
plt.show()

"""test 3"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# คำนวณ correlation matrix
corr_matrix = df.corr()

# สร้าง heatmap
plt.figure(figsize=(12, 10))  # กำหนดขนาดของ heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""test 4"""

print(df.columns)  # Print the column names of your DataFrame

import seaborn as sns
import matplotlib.pyplot as plt

# กำหนด feature names
feature_names = ['Open', 'High', 'Low', 'Vol.', 'Change %', 'Year', 'Month', 'Day', 'Season_Autumn', 'Season_Spring', 'Season_Summer', 'Season_Winter']

# สร้าง pairplot
sns.pairplot(df, x_vars=feature_names, y_vars=['Price'], kind='scatter', diag_kind='hist')
plt.suptitle('Scatter Plots and Histograms of Bitcoin Data', y=1.02)
plt.show()

# from flask import Flask, request, jsonify
# import pandas as pd
# import joblib  # or pickle

# # โหลด model ที่ train แล้ว
# model = joblib.load('best_model_bitcoin.pkl')  # เปลี่ยนชื่อไฟล์ model ของคุณ

# app = Flask(__name__)

# @app.route('/predict', methods=['POST'])
# def predict():
#     data = request.get_json()  # รับข้อมูล input เป็น JSON

#     # ดึง features ที่ต้องการจาก JSON data
#     year = data['Year']
#     month = data['Month']
#     day = data['Day']
#     season_autumn = data['Season_Autumn']
#     season_spring = data['Season_Spring']
#     season_summer = data['Season_Summer']
#     season_winter = data['Season_Winter']

#     # สร้าง DataFrame จาก features
#     input_data = pd.DataFrame([[year, month, day, season_autumn, season_spring, season_summer, season_winter]],
#                               columns=['Year', 'Month', 'Day', 'Season_Autumn', 'Season_Spring', 'Season_Summer', 'Season_Winter'])

#     # ทำนายราคา Bitcoin
#     predicted_price = model.predict(input_data)[0]

#     # ส่งผลลัพธ์เป็น JSON
#     return jsonify({'predicted_price': predicted_price})

# if __name__ == '__main__':
#     app.run(debug=True)